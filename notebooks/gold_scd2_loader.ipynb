{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3bc750f",
   "metadata": {},
   "source": [
    "# Gold loader: promover Silver -> Gold\n",
    "\n",
    "Este notebook lê tabelas Delta da camada `silver` e grava tabelas na camada `gold` usando os prefixos `dim_` (dimensões) e `fact_` (fatos).\n",
    "\n",
    "Notas:\n",
    "- Mantém convenções de nome do catálogo e schema (ex.: `mkl_bank.default.silver_<table>`).\n",
    "- Em ambientes sem Databricks, passos de otimização (OPTIMIZE / ZORDER) serão ignorados.\n",
    "- Execute este notebook em um ambiente com Spark e Delta habilitados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c9e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports e configuração\n",
    "from pyspark.sql.functions import current_timestamp, col, to_date, lit, monotonically_increasing_id\n",
    "import pyspark.sql.functions as F\n",
    "import uuid\n",
    "\n",
    "# Garantir sessão Spark disponível (em Databricks já existe 'spark')\n",
    "catalog_name = \"mkl_bank\"\n",
    "silver_schema = \"default\"\n",
    "gold_schema = \"gold\"\n",
    "\n",
    "# Opções de escrita comuns\n",
    "WRITE_FORMAT = \"delta\"\n",
    "WRITE_MODE = \"overwrite\"  # conforme especificado no outline\n",
    "\n",
    "# Identificador de execução / carga\n",
    "batch_id = str(uuid.uuid4())\n",
    "load_ts = current_timestamp()\n",
    "\n",
    "print(f\"Gold loader initialized. catalog={catalog_name}, silver_schema={silver_schema}, gold_schema={gold_schema}, batch_id={batch_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear tabelas silver -> gold (nome e tipo)\n",
    "mapping = {\n",
    "    'silver_agencia': ('dim_agencia', 'dim'),\n",
    "    'silver_cliente': ('dim_cliente', 'dim'),\n",
    "    'silver_conta': ('dim_conta', 'dim'),\n",
    "    'silver_cartao': ('dim_cartao', 'dim'),\n",
    "    'silver_chave_pix': ('dim_chave_pix', 'dim'),\n",
    "    'silver_transacao_conta': ('fact_transacao', 'fact'),\n",
    "    'silver_movimentacao_transferencia_realizada': ('fact_transferencia_realizada', 'fact'),\n",
    "    'silver_movimentacao_transferencia_recebida': ('fact_transferencia_recebida', 'fact'),\n",
    "    'silver_movimentacao_pix_realizado': ('fact_pix_realizado', 'fact'),\n",
    "    'silver_movimentacao_pix_recebido': ('fact_pix_recebido', 'fact'),\n",
    "    'silver_movimentacao_deposito_recebido': ('fact_deposito_recebido', 'fact'),\n",
    "    'silver_movimentacao_pagamento_cartao': ('fact_pagamento_cartao', 'fact'),\n",
    "    'silver_movimentacao_boleto_pago': ('fact_boleto_pago', 'fact')\n",
    "}\n",
    "\n",
    "# Chaves naturais/PK esperadas para dimensões (usar os IDs conforme indicado)\n",
    "# O usuário pediu para considerar as chaves naturais como os campos id_* conforme os schemas\n",
    "natural_keys = {\n",
    "    'dim_agencia': 'codigo_agencia',   # agência usa código\n",
    "    'dim_cliente': 'id_cliente',        # cliente: id_cliente\n",
    "    'dim_conta': 'id_conta',            # conta: id_conta\n",
    "    'dim_cartao': 'id_cartao',          # cartao: id_cartao\n",
    "    'dim_chave_pix': 'id_chave_pix'     # chave pix: id_chave_pix\n",
    "}\n",
    "\n",
    "# Lista de tabelas a processar (por conveniência)\n",
    "tables_to_process = list(mapping.keys())\n",
    "print(f\"Tables to process: {tables_to_process}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler tabelas Silver para DataFrames (sem cache para evitar PERSIST TABLE em serverless)\n",
    "dfs = {}\n",
    "for silver_name in tables_to_process:\n",
    "    full_name = f\"{catalog_name}.{silver_schema}.{silver_name}\"\n",
    "    try:\n",
    "        if not spark.catalog.tableExists(full_name):\n",
    "            print(f\"Table {full_name} does not exist, skipping\")\n",
    "            dfs[silver_name] = None\n",
    "            continue\n",
    "\n",
    "        # leitura direta sem cache/persist\n",
    "        df = spark.table(full_name)\n",
    "        dfs[silver_name] = df\n",
    "\n",
    "        # contar linhas é opcional; em ambientes serverless pode ser custoso, mas tentamos com tratamento\n",
    "        try:\n",
    "            cnt = df.count()\n",
    "        except Exception as e_count:\n",
    "            cnt = None\n",
    "            print(f\"Warning counting rows for {full_name}: {e_count}\")\n",
    "\n",
    "        print(f\"Loaded {full_name} -> {cnt} rows\")\n",
    "    except Exception as e:\n",
    "        dfs[silver_name] = None\n",
    "        print(f\"Warning: could not load {full_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d25fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar e carregar dimensões (dim_)\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "for silver_name, (gold_name, typ) in mapping.items():\n",
    "    if typ != 'dim':\n",
    "        continue\n",
    "\n",
    "    df = dfs.get(silver_name)\n",
    "    if df is None:\n",
    "        print(f\"Skipping {silver_name}: no DataFrame\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing dimension {silver_name} -> {gold_name}\")\n",
    "    nat_key = natural_keys.get(gold_name)\n",
    "\n",
    "    # dedupe por chave natural quando disponível\n",
    "    if nat_key and nat_key in df.columns:\n",
    "        df_out = df.dropDuplicates([nat_key])\n",
    "    else:\n",
    "        df_out = df.dropDuplicates()\n",
    "\n",
    "    # adicionar surrogate key e timestamp de carga\n",
    "    df_out = df_out.withColumn('surrogate_id', monotonically_increasing_id()).withColumn('load_datetime', current_timestamp())\n",
    "\n",
    "    # Normalização específica: criar chave canônica para cartão (cartao_key)\n",
    "    # Isso permite usar numero_cartao ou id_cartao nos fatos e juntar consistentemente.\n",
    "    if gold_name == 'dim_cartao':\n",
    "        df_out = df_out.withColumn('cartao_key', F.coalesce(col('numero_cartao'), col('id_cartao').cast('string')))\n",
    "\n",
    "    target_table = f\"{catalog_name}.{gold_schema}.{gold_name}\"\n",
    "    try:\n",
    "        df_out.write.format(WRITE_FORMAT).mode(WRITE_MODE).option('overwriteSchema','true').saveAsTable(target_table)\n",
    "        print(f\"Wrote dimension table {target_table} (rows={spark.table(target_table).count()})\")\n",
    "    except AnalysisException as e:\n",
    "        print(f\"Failed to write {target_table}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error writing {target_table}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0258ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar e carregar fatos (fact_) com particionamento por event_date quando aplicável\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def enrich_with_dim(df_fact: DataFrame, dim_table: str, fact_cols_candidates: list, dim_cols_candidates: list, fk_alias: str) -> DataFrame:\n",
    "    \"\"\"Tenta encontrar uma coluna em comum entre df_fact e dim_table para fazer o join\n",
    "    - fact_cols_candidates: lista de nomes esperados no fato (prioridade)\n",
    "    - dim_cols_candidates: lista de nomes esperados na dimensão (prioridade)\n",
    "    - fk_alias: nome da coluna surrogate id a ser criada no fato (ex: 'conta_sk')\n",
    "    \"\"\"\n",
    "    # verificar se fato tem pelo menos uma das colunas candidatas\n",
    "    fact_col = next((c for c in fact_cols_candidates if c in df_fact.columns), None)\n",
    "    if fact_col is None:\n",
    "        return df_fact\n",
    "\n",
    "    if not spark.catalog.tableExists(dim_table):\n",
    "        print(f\"{dim_table} not found, skipping {fk_alias}\")\n",
    "        return df_fact\n",
    "\n",
    "    dim_df = spark.table(dim_table)\n",
    "\n",
    "    # escolher coluna da dimensão para juntar\n",
    "    if fact_col in dim_df.columns:\n",
    "        dim_key = fact_col\n",
    "    else:\n",
    "        dim_key = next((c for c in dim_cols_candidates if c in dim_df.columns), None)\n",
    "\n",
    "    if dim_key is None:\n",
    "        print(f\"No suitable join key found between fact and {dim_table} for {fk_alias} (fact_col={fact_col})\")\n",
    "        return df_fact\n",
    "\n",
    "    # Evitar colisão de nomes de colunas renomeando a coluna da dimensão antes do join\n",
    "    dim_prefixed = f\"dim_{dim_key}\"\n",
    "    dim_sel = dim_df.select(col(dim_key).alias(dim_prefixed), col('surrogate_id').alias(fk_alias))\n",
    "\n",
    "    # Fazer join usando a coluna renomeada\n",
    "    df_joined = df_fact.join(dim_sel, df_fact[fact_col] == dim_sel[dim_prefixed], how='left')\n",
    "\n",
    "    # Remover coluna temporária da dimensão\n",
    "    if dim_prefixed in df_joined.columns:\n",
    "        df_joined = df_joined.drop(dim_prefixed)\n",
    "\n",
    "    return df_joined\n",
    "\n",
    "\n",
    "for silver_name, (gold_name, typ) in mapping.items():\n",
    "    if typ != 'fact':\n",
    "        continue\n",
    "\n",
    "    df = dfs.get(silver_name)\n",
    "    if df is None:\n",
    "        print(f\"Skipping {silver_name}: no DataFrame\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing fact {silver_name} -> {gold_name}\")\n",
    "    df_fact = df\n",
    "\n",
    "    # tentar extrair data do evento\n",
    "    if 'data_transacao' in df_fact.columns:\n",
    "        df_fact = df_fact.withColumn('event_date', to_date(col('data_transacao')))\n",
    "    elif 'data' in df_fact.columns:\n",
    "        df_fact = df_fact.withColumn('event_date', to_date(col('data')))\n",
    "    else:\n",
    "        df_fact = df_fact.withColumn('event_date', lit(None).cast('date'))\n",
    "\n",
    "    # dedupe por id_transacao quando disponível\n",
    "    if 'id_transacao' in df_fact.columns:\n",
    "        df_fact = df_fact.dropDuplicates(['id_transacao'])\n",
    "\n",
    "    df_fact = df_fact.withColumn('load_datetime', current_timestamp())\n",
    "\n",
    "    # --- Normalizar e enriquecer fatos com chaves estrangeiras (surrogate_id das dimensões) ---\n",
    "    # Normalizar cartão: criar cartao_key no fato também (coalesce)\n",
    "    if 'numero_cartao' in df_fact.columns or 'id_cartao' in df_fact.columns:\n",
    "        df_fact = df_fact.withColumn('cartao_key', F.coalesce(col('numero_cartao'), col('id_cartao').cast('string')))\n",
    "\n",
    "    # Conta: id_conta, id_conta\n",
    "    df_fact = enrich_with_dim(\n",
    "        df_fact,\n",
    "        f\"{catalog_name}.{gold_schema}.dim_conta\",\n",
    "        ['id_conta', 'id_conta'],\n",
    "        ['id_conta', 'id_conta'],\n",
    "        'conta_sk'\n",
    "    )\n",
    "\n",
    "    # Cartão: preferir cartao_key criado; suporte numero_cartao/id_cartao como fallback\n",
    "    df_fact = enrich_with_dim(\n",
    "        df_fact,\n",
    "        f\"{catalog_name}.{gold_schema}.dim_cartao\",\n",
    "        ['cartao_key', 'numero_cartao', 'id_cartao'],\n",
    "        ['cartao_key', 'numero_cartao', 'id_cartao'],\n",
    "        'cartao_sk'\n",
    "    )\n",
    "\n",
    "    # Chave PIX: id_chave_pix, valor_chave\n",
    "    df_fact = enrich_with_dim(\n",
    "        df_fact,\n",
    "        f\"{catalog_name}.{gold_schema}.dim_chave_pix\",\n",
    "        ['id_chave_pix', 'valor_chave'],\n",
    "        ['id_chave_pix', 'valor_chave'],\n",
    "        'chave_pix_sk'\n",
    "    )\n",
    "\n",
    "    # Cliente: id_cliente, cpf\n",
    "    df_fact = enrich_with_dim(\n",
    "        df_fact,\n",
    "        f\"{catalog_name}.{gold_schema}.dim_cliente\",\n",
    "        ['id_cliente', 'cpf'],\n",
    "        ['id_cliente', 'cpf'],\n",
    "        'cliente_sk'\n",
    "    )\n",
    "\n",
    "    # Agência: codigo_agencia\n",
    "    df_fact = enrich_with_dim(\n",
    "        df_fact,\n",
    "        f\"{catalog_name}.{gold_schema}.dim_agencia\",\n",
    "        ['codigo_agencia'],\n",
    "        ['codigo_agencia'],\n",
    "        'agencia_sk'\n",
    "    )\n",
    "\n",
    "    # Escrever fato enriquecido\n",
    "    target_table = f\"{catalog_name}.{gold_schema}.{gold_name}\"\n",
    "    try:\n",
    "        if 'event_date' in df_fact.columns:\n",
    "            df_fact.write.format(WRITE_FORMAT).mode(WRITE_MODE).option('overwriteSchema','true').partitionBy('event_date').saveAsTable(target_table)\n",
    "        else:\n",
    "            df_fact.write.format(WRITE_FORMAT).mode(WRITE_MODE).option('overwriteSchema','true').saveAsTable(target_table)\n",
    "        print(f\"Wrote fact table {target_table} (rows={spark.table(target_table).count()})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {target_table}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificação de counts, checagens básicas de qualidade e integridade referencial\n",
    "for silver_name, (gold_name, typ) in mapping.items():\n",
    "    silver_full = f\"{catalog_name}.{silver_schema}.{silver_name}\"\n",
    "    gold_full = f\"{catalog_name}.{gold_schema}.{gold_name}\"\n",
    "    try:\n",
    "        s_cnt = spark.table(silver_full).count()\n",
    "    except Exception:\n",
    "        s_cnt = None\n",
    "    try:\n",
    "        g_cnt = spark.table(gold_full).count()\n",
    "    except Exception:\n",
    "        g_cnt = None\n",
    "\n",
    "    print(f\"{silver_full} -> silver_count={s_cnt}; {gold_full} -> gold_count={g_cnt}\")\n",
    "\n",
    "    # checar nulos nas chaves naturais para dimensões\n",
    "    if typ == 'dim':\n",
    "        pk = natural_keys.get(gold_name)\n",
    "        if pk and spark.catalog.tableExists(gold_full):\n",
    "            nulls = spark.table(gold_full).filter(col(pk).isNull()).count()\n",
    "            print(f\"Null key count for {gold_full}.{pk}: {nulls}\")\n",
    "\n",
    "    # checar integridade referencial mínima para fatos (FK surrogate ids criadas)\n",
    "    if typ == 'fact':\n",
    "        try:\n",
    "            gdf = spark.table(gold_full)\n",
    "            fk_candidates = [c for c in ['conta_sk','cartao_sk','chave_pix_sk','cliente_sk','agencia_sk'] if c in gdf.columns]\n",
    "            for fk in fk_candidates:\n",
    "                nnull = gdf.filter(col(fk).isNull()).count()\n",
    "                print(f\"FK nulls {gold_full}.{fk}: {nnull}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not inspect FK columns for {gold_full}: {e}\")\n",
    "\n",
    "print(\"Verificações concluídas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de931e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: otimização/compactação das tabelas Gold (Databricks)\n",
    "for silver_name, (gold_name, typ) in mapping.items():\n",
    "    gold_full = f\"{catalog_name}.{gold_schema}.{gold_name}\"\n",
    "    try:\n",
    "        print(f\"Attempting OPTIMIZE on {gold_full}\")\n",
    "        spark.sql(f\"OPTIMIZE {gold_full}\")\n",
    "        # z-order on natural key when available\n",
    "        zkey = natural_keys.get(gold_name)\n",
    "        if zkey:\n",
    "            spark.sql(f\"OPTIMIZE {gold_full} ZORDER BY ({zkey})\")\n",
    "    except Exception as e:\n",
    "        print(f\"OPTIMIZE skipped or not supported for {gold_full}: {e}\")\n",
    "\n",
    "print(\"Optional optimization step finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
